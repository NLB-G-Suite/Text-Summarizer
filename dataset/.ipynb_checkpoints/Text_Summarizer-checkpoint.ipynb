{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "from load_data import get_lookup_tables, filter_len, word2int, find_longest_sequence, limits, start_token, end_token\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tf version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 120\n",
    "lstm_state_size = 300\n",
    "vocab_size = limits['vocab_size']\n",
    "batch_size = 64\n",
    "validation_split = 0.15\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, summary = filter_len()\n",
    "#word2int_lookup, int2word_lookup = get_lookup_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize(Tokenizer):\n",
    "    def __init__(self, text, vocab_size):\n",
    "        Tokenizer.__init__(self, num_words=vocab_size)\n",
    "        self.fit_on_texts(text)\n",
    "        self.int2word = {v:k for (k,v) in self.word_index.items()}\n",
    "        self.word2int = self.word_index\n",
    "        \n",
    "    def int2word_lookup(self, int_list):\n",
    "        words = [self.int2word[integer] for integer in int_list if integer != 0]\n",
    "        \n",
    "        return \" \".join(words)\n",
    "        \n",
    "    def word2int_lookup(self, word_list):\n",
    "        return self.texts_to_sequences(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = Tokenize(text, limits['vocab_size'])\n",
    "summary_tokenizer = Tokenize(summary, limits['vocab_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sssss syrian official obama climbed to the top of the tree doesnt know how to get down obama sends a letter to the heads of the house and senate obama to seek congressional approval on military action against syria aim is to determine whether cw were used not by whom says un spokesman eeeee'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_tokenizer.word2int_lookup(text)\n",
    "summary = summary_tokenizer.word2int_lookup(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 682,\n",
       " 77,\n",
       " 60,\n",
       " 8130,\n",
       " 2,\n",
       " 1,\n",
       " 140,\n",
       " 6,\n",
       " 1,\n",
       " 3641,\n",
       " 460,\n",
       " 414,\n",
       " 159,\n",
       " 2,\n",
       " 126,\n",
       " 130,\n",
       " 60,\n",
       " 2800,\n",
       " 7,\n",
       " 1129,\n",
       " 2,\n",
       " 1,\n",
       " 2142,\n",
       " 6,\n",
       " 1,\n",
       " 112,\n",
       " 8,\n",
       " 398,\n",
       " 60,\n",
       " 2,\n",
       " 1225,\n",
       " 1965,\n",
       " 2049,\n",
       " 12,\n",
       " 107,\n",
       " 467,\n",
       " 75,\n",
       " 299,\n",
       " 2801,\n",
       " 10,\n",
       " 2,\n",
       " 2006,\n",
       " 407,\n",
       " 36,\n",
       " 165,\n",
       " 33,\n",
       " 26,\n",
       " 5099,\n",
       " 9,\n",
       " 212,\n",
       " 245,\n",
       " 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Debugging Tokenizer Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "1: to\n",
      "2: sssss\n",
      "3: eeeee\n",
      "4: in\n",
      "5: of\n",
      "6: a\n",
      "7: and\n",
      "8: says\n",
      "9: is\n"
     ]
    }
   ],
   "source": [
    "for (i, entry) in enumerate(summary_tokenizer.word2int):\n",
    "    if i < 10:\n",
    "        print(\"{}: {}\".format(i,entry))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "1: to\n",
      "2: of\n",
      "3: and\n",
      "4: a\n",
      "5: in\n",
      "6: that\n",
      "7: for\n",
      "8: is\n",
      "9: said\n"
     ]
    }
   ],
   "source": [
    "for (i, entry) in enumerate(text_tokenizer.word2int):\n",
    "    if i < 10:\n",
    "        print(\"{}: {}\".format(i,entry))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the Data\n",
    "\n",
    "Zero pad text and summary. \n",
    "\n",
    "Padding the text: Get list of number of words for every text, use 2 standard deviations from the mean, using this length should cover 95% of all texts\n",
    "\n",
    "Padding the summary: Find the longest summary, pad all summaries to that length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest text has 1224 words\n",
      "using maximum of 1224.0 words\n",
      "this will cover 0.0 of input text\n"
     ]
    }
   ],
   "source": [
    "num_words_per_text = [len(word) for word in text]\n",
    "\n",
    "longest_text = find_longest_sequence(text)\n",
    "max_words_text = np.mean(num_words_per_text) + 2 * np.std(num_words_per_text)\n",
    "\n",
    "print(\"longest text has {} words\".format(longest_text))\n",
    "print(\"using maximum of {} words\".format(max_words_text))\n",
    "print(\"this will cover {} of input text\".format(np.sum(num_words_per_text < max_words_text) / len(num_words_per_text)))\n",
    "\n",
    "max_words_text = int(max_words_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91471, 1224)\n",
      "(91471, 91)\n"
     ]
    }
   ],
   "source": [
    "text = pad_sequences(text, maxlen=max_words_text, padding='pre')\n",
    "\n",
    "longest_summary = find_longest_sequence(summary)\n",
    "summary = pad_sequences(summary, maxlen=longest_summary, padding='post')\n",
    "\n",
    "print(text.shape)\n",
    "print(summary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0   46 8081 8740  141    1   88 3954  247   26\n",
      " 2720   18  522 1397    6 2197   16   13 9304    2  744    6    1 3104\n",
      " 8642    1 5112  182    6    1   88  751  391    3  104  102 2703 4056\n",
      "   16    1    3 2737    4 8081  424    6 1970    1   50 1845  225    6\n",
      " 1970   14 1941  376    1 5590   49 1766   34    8    5    1 6986 8081\n",
      "   27   86 3665  699 1397 5215   17   88 3954    1  546  399   26  170\n",
      " 8278 4890 3014  578 1466    4   29    2 2806    1  405  674    3  349\n",
      " 1411 2715    1 8642 4219 1037 1402 7794    6    1  703    4 1547 2330\n",
      "    6    1  687  615  183 1877    3 1406    4 1279  459    2  144    2\n",
      " 7629    8   16  205   16  433 8081   10  344 1399  762   18 4486    2\n",
      " 1452   11  271    1 2388 3690 1888  744   12  184 1974    6 1764  319\n",
      "   13  238    1 3889    6  812   22  100    4    1  104  102  522 2225\n",
      " 7396   40  604   56    3   18 4653   16   13 3288    2   99  515  367\n",
      "    3   36    4   12  184  318    2   99   11 1086   14 8081  341  181\n",
      "   36 9066    6    1 7346  791   26 1124    1 8642 1397   26   32   73\n",
      " 8081   12 7247    5 6137   36 2398    5 2053  546    3 1970  138  391\n",
      "    3  820   32 3730    1  518    6  225  196    6 1970 2735 1217    1\n",
      "  104  102   34 1858  111    6    1 5590 2631  923   49 1684   67   11\n",
      "    1  225  148 2679    4 1095 8497   24 5913    2 2898   48  820   34\n",
      " 5387    8   28 1146    1  472   32   34 1858 1287   34 5913    2    1\n",
      " 5590   54   36 3104  209   73 8081 5677 1681  475    1   76  363    2\n",
      " 2617  118    6    1    4    1 8642    6   68  423  491   11    1   81\n",
      "  112    3    1 3954 4322  475    1  522  182    2 7266   77    6    1\n",
      " 6137 3453    1  861   26  200    2  134 1397 5036 7653 1057  212 1397\n",
      "   17  651  706    6    1 1424   49  241 1071  100 8706 2222  424    5\n",
      " 8343 3104  423 6410 7826   12    5 2002 1346    3    1 1424  423    8\n",
      " 2807 6147  717    1 2053    2    5 6812   24  100    1 3809 6639    1\n",
      " 2631 1641   22    1  104  102   52   12 1820    3    1 1259    6    1\n",
      " 1926   52   12  621 1237   48   36 5417 1397  119 1770 2149    6  226\n",
      "    3 2454   57  943   54  115    1    3  951 1265  267 6357    5  792\n",
      " 4884   38  210 2116    2 9723   41  973   24   52   34 4961  758   26\n",
      " 1144  223  335    8    5 6929    3    1 1166 1642  549    6 4973    1\n",
      "  185  365  989  573    6  617]\n"
     ]
    }
   ],
   "source": [
    "print(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3   15    7 1888  819    9   35   13  163    6    7  150 9476 1799\n",
      "    5  498   15 5881 3531 1839 5148 2790 3519    8 2679   10   64  802\n",
      "    5    7  805 5593 1349   78   31  582  607    1  805  148  160    7\n",
      " 4428    6 2256    5  966 1083    4    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(summary[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function, decode integers\n",
    "\n",
    "This function takes as input a list of words represented in integers and translates these integers into the corresponding string using the int2word dictionary that was created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sssss new a canadian doctor says she was part of a team examining harry in 2010 new diagnosis autism severe anxiety stress disorder and depression is also suspected in a german arson probe officials say prosecutors believe the german national set a string of fires in los angeles eeeee'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_tokenizer.int2word_lookup(summary[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training (Teacher Forcing)\n",
    "\n",
    "For every pair of text and summary, the encoder will create a final state that captures the contextual information present in the input text. The decoder will then use this final state emitted from the encoder to predict the target sequence. The decoder reads the entire target sequence word by word and predict the same sequence offset by one timestep. The decoder is trained to predict the next word in the sequence given the previous word.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
    "https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = text\n",
    "decoder_input_data = summary[:, :-1]\n",
    "decoder_output_data = summary[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,  682,   77,   60, 8130,    2,    1,  140,    6,    1, 3641,\n",
       "        460,  414,  159,    2,  126,  130,   60, 2800,    7, 1129,    2,\n",
       "          1, 2142,    6,    1,  112,    8,  398,   60,    2, 1225, 1965,\n",
       "       2049,   12,  107,  467,   75,  299, 2801,   10,    2, 2006,  407,\n",
       "         36,  165,   33,   26, 5099,    9,  212,  245,    4,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 682,   77,   60, 8130,    2,    1,  140,    6,    1, 3641,  460,\n",
       "        414,  159,    2,  126,  130,   60, 2800,    7, 1129,    2,    1,\n",
       "       2142,    6,    1,  112,    8,  398,   60,    2, 1225, 1965, 2049,\n",
       "         12,  107,  467,   75,  299, 2801,   10,    2, 2006,  407,   36,\n",
       "        165,   33,   26, 5099,    9,  212,  245,    4,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = {\n",
    "    'encoder_input' : encoder_input_data,\n",
    "    'decoder_input' : decoder_input_data\n",
    "}\n",
    "\n",
    "target_data = {\n",
    "    'decoder_output' : decoder_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None,),name=\"encoder_input\")\n",
    "encoder_embedding = Embedding(input_dim=vocab_size+4,\n",
    "                              output_dim=embedding_size,\n",
    "                              name=\"encoder_embedding\")\n",
    "\n",
    "encoder_hidden1 = GRU(lstm_state_size, return_sequences=True, name=\"encoder_hid1\")\n",
    "encoder_hidden2 = GRU(lstm_state_size, return_sequences=True, name=\"encoder_hid2\")\n",
    "encoder_hidden3 = GRU(lstm_state_size, return_sequences=False, name=\"encoder_hid3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoding_layer():\n",
    "    # connect input with embedding\n",
    "    output = encoder_embedding(encoder_input)\n",
    "    \n",
    "    # connect embedding with hidden layers\n",
    "    output = encoder_hidden1(output)\n",
    "    output = encoder_hidden2(output)\n",
    "    output = encoder_hidden3(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_states = connect_encoding_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None,), name=\"decoder_input\")\n",
    "decoder_embedding = Embedding(input_dim = vocab_size+4,\n",
    "                              output_dim = embedding_size,\n",
    "                              name=\"decoder_embedding\")\n",
    "\n",
    "decoder_hidden1 = GRU(lstm_state_size, return_sequences=True, name=\"decoder_hid1\")\n",
    "decoder_hidden2 = GRU(lstm_state_size, return_sequences=True, name=\"decoder_hid2\")\n",
    "decoder_hidden3 = GRU(lstm_state_size, return_sequences=True, name=\"decoder_hid3\")\n",
    "\n",
    "decoder_dense = Dense(vocab_size,\n",
    "                      activation='softmax',\n",
    "                      name='decoder_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_decoding_layer(decoder_init_state):\n",
    "    # connect input with embedding\n",
    "    output = decoder_embedding(decoder_input)\n",
    "    \n",
    "    # connect embedding with hidden layers\n",
    "    output = decoder_hidden1(output, initial_state=decoder_init_state)\n",
    "    output = decoder_hidden2(output, initial_state=decoder_init_state)\n",
    "    output = decoder_hidden3(output, initial_state=decoder_init_state)\n",
    "\n",
    "    output = decoder_dense(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoding_layer(encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the Encoder and Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input],\n",
    "              outputs=[decoder_output])\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-3), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoint/checkpoint.keras\"\n",
    "callback_checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in text:\n",
    "    for integer in entry:\n",
    "        if integer not in int2word_lookup:\n",
    "            print(integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=source_data,\n",
    "          y=target_data,\n",
    "          batch_size=batch_size,\n",
    "          validation_split=validation_split,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Mode\n",
    "\n",
    "First setup the encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                      outputs=[encoder_states])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(lstm_state_size,), \n",
    "                              name='decoder_state_input_h')\n",
    "decoder_state_input_c = Input(shape=(lstm_state_size,), \n",
    "                              name='decoder_state_input_c')\n",
    "decoder_hidden_state_input = Input(shape=(max_words_text,lstm_state_size))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_output, decoder_state_output_h, decoder_state_output_c = connect_decoding_layer(decoder_state_inputs)\n",
    "\n",
    "model_deocder = Model(inputs=[decoder_input,decoder_hidden_state_input] + decoder_state_inputs,\n",
    "                      outputs=[decoder_output, decoder_state_output_h, decoder_state_output_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function Used to Predict the Summary of Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_text):\n",
    "    input_ints = word2int(input_text, word2int_lookup)\n",
    "    input_ints = pad_sequences(input_ints, maxlen=int(max_words_text), padding='pre')\n",
    "    \n",
    "    # Obtain the output and states of the encoder using the input text\n",
    "    encoder_out, encoder_state_h, encoder_state_c = model_encoder.predict(input_ints)\n",
    "    encoder_outputs = [encoder_out, encoder_state_h, encoder_state_c]\n",
    "   \n",
    "    # Data that will be fed into the decoder and initialize sample token to start token\n",
    "    decoder_input_data = np.zeros(shape=(1,max_words_text), dtype=np.int)\n",
    "    sampled_token_int = word2int_lookup[start_token]\n",
    "    \n",
    "    # Initialize predicted text and keep track of number of words processed\n",
    "    predicted_summary = list()\n",
    "    predicted_summary.append(start_token)\n",
    "    count_tokens = 0\n",
    "    \n",
    "    while sampled_token_int != word2int_lookup[end_token] and count_tokens < max_words_text:\n",
    "        # add the next token to the input data\n",
    "        decoder_input_data[0, count_tokens] = sampled_token_int\n",
    "        \n",
    "        input_data = {\n",
    "            'decoder_initial_state' : encoder_outputs,\n",
    "            'decoder_input' : decoder_input_data\n",
    "        }\n",
    "        \n",
    "        # use decoder to get output tokens\n",
    "        output_tokens, _, _ = model_decoder.predict(input_data)\n",
    "        \n",
    "        # get last predicted token as one hot array\n",
    "        last_token = output_tokens[0, count_tokens, :]\n",
    "        \n",
    "        # convert to int\n",
    "        sampled_token_int = np.argmax(last_token)\n",
    "        \n",
    "        # convert to word\n",
    "        sampled_word = int2word_lookup[sampled_token_int]\n",
    "        \n",
    "        # add to predicted summary\n",
    "        predicted_summary.append(sampled_word)\n",
    "        count_tokens += 1\n",
    "    \n",
    "    predicted_summary = \" \".join(predicted_summary)\n",
    "    \n",
    "    print(predicted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
