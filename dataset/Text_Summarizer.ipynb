{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "from load_data import get_lookup_tables, filter_len, word2int, find_longest_sequence, limits, start_token, end_token\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tf version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 120\n",
    "lstm_state_size = 300\n",
    "vocab_size = limits['vocab_size']\n",
    "batch_size = 64\n",
    "validation_split = 0.15\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, summary = filter_len()\n",
    "#word2int_lookup, int2word_lookup = get_lookup_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize(Tokenizer):\n",
    "    def __init__(self, text, vocab_size):\n",
    "        Tokenizer.__init__(self, num_words=vocab_size)\n",
    "        self.fit_on_texts(text)\n",
    "        self.int2word = {v:k for (k,v) in self.word_index.items()}\n",
    "        self.word2int = self.word_index\n",
    "        \n",
    "    def int2word_lookup(self, int_list):\n",
    "        words = [self.int2word[integer] for integer in int_list if integer != 0]\n",
    "        \n",
    "        return \" \".join(words)\n",
    "        \n",
    "    def word2int_lookup(self, word_list):\n",
    "        return self.texts_to_sequences(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = Tokenize(text, limits['vocab_size'])\n",
    "summary_tokenizer = Tokenize(summary, limits['vocab_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_tokenizer.word2int_lookup(text)\n",
    "summary = summary_tokenizer.word2int_lookup(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Debugging Tokenizer Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tokenizer.word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer.word2int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the Data\n",
    "\n",
    "Zero pad text and summary. \n",
    "\n",
    "Padding the text: Get list of number of words for every text, use 2 standard deviations from the mean, using this length should cover 95% of all texts\n",
    "\n",
    "Padding the summary: Find the longest summary, pad all summaries to that length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_per_text = [len(word) for word in text]\n",
    "\n",
    "longest_text = find_longest_sequence(text)\n",
    "max_words_text = np.mean(num_words_per_text) + 2 * np.std(num_words_per_text)\n",
    "\n",
    "print(\"longest text has {} words\".format(longest_text))\n",
    "print(\"using maximum of {} words\".format(max_words_text))\n",
    "print(\"this will cover {} of input text\".format(np.sum(num_words_per_text < max_words_text) / len(num_words_per_text)))\n",
    "\n",
    "max_words_text = int(max_words_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pad_sequences(text, maxlen=max_words_text, padding='pre')\n",
    "\n",
    "longest_summary = find_longest_sequence(summary)\n",
    "summary = pad_sequences(summary, maxlen=longest_summary, padding='post')\n",
    "\n",
    "print(text.shape)\n",
    "print(summary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_tokenizer.int2word_lookup(text[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function, decode integers\n",
    "\n",
    "This function takes as input a list of words represented in integers and translates these integers into the corresponding string using the int2word dictionary that was created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer.int2word_lookup(text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training (Teacher Forcing)\n",
    "\n",
    "For every pair of text and summary, the encoder will create a final state that captures the contextual information present in the input text. The decoder will then use this final state emitted from the encoder to predict the target sequence. The decoder reads the entire target sequence word by word and predict the same sequence offset by one timestep. The decoder is trained to predict the next word in the sequence given the previous word.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
    "https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = text[1:]\n",
    "decoder_input_data = summary[1:, :-1]\n",
    "decoder_output_data = summary[1:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = {\n",
    "    'encoder_input' : encoder_input_data,\n",
    "    'decoder_input' : decoder_input_data\n",
    "}\n",
    "\n",
    "target_data = {\n",
    "    'decoder_output' : decoder_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None,),name=\"encoder_input\")\n",
    "encoder_embedding = Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embedding_size,\n",
    "                              name=\"encoder_embedding\")\n",
    "\n",
    "encoder_hidden1 = GRU(lstm_state_size, return_sequences=True, name=\"encoder_hid1\")\n",
    "encoder_hidden2 = GRU(lstm_state_size, return_sequences=True, name=\"encoder_hid2\")\n",
    "encoder_hidden3 = GRU(lstm_state_size, return_sequences=False, name=\"encoder_hid3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoding_layer():\n",
    "    # connect input with embedding\n",
    "    output = encoder_embedding(encoder_input)\n",
    "    \n",
    "    # connect embedding with hidden layers\n",
    "    output = encoder_hidden1(output)\n",
    "    output = encoder_hidden2(output)\n",
    "    output = encoder_hidden3(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_states = connect_encoding_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None,), name=\"decoder_input\")\n",
    "decoder_embedding = Embedding(input_dim = vocab_size,\n",
    "                              output_dim = embedding_size,\n",
    "                              name=\"decoder_embedding\")\n",
    "\n",
    "decoder_hidden1 = GRU(lstm_state_size, return_sequences=True, name=\"decoder_hid1\")\n",
    "decoder_hidden2 = GRU(lstm_state_size, return_sequences=True, name=\"decoder_hid2\")\n",
    "decoder_hidden3 = GRU(lstm_state_size, return_sequences=True, name=\"decoder_hid3\")\n",
    "\n",
    "decoder_dense = Dense(vocab_size,\n",
    "                      activation='softmax',\n",
    "                      name='decoder_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_decoding_layer(decoder_init_state):\n",
    "    # connect input with embedding\n",
    "    output = decoder_embedding(decoder_input)\n",
    "    \n",
    "    # connect embedding with hidden layers\n",
    "    output = decoder_hidden1(output, initial_state=decoder_init_state)\n",
    "    output = decoder_hidden2(output, initial_state=decoder_init_state)\n",
    "    output = decoder_hidden3(output, initial_state=decoder_init_state)\n",
    "\n",
    "    output = decoder_dense(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoding_layer(encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the Encoder and Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input],\n",
    "              outputs=[decoder_output])\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-3), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoint/checkpoint.keras\"\n",
    "callback_checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=source_data,\n",
    "          y=target_data,\n",
    "          batch_size=batch_size,\n",
    "          validation_split=validation_split,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Mode\n",
    "\n",
    "First setup the encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                      outputs=[encoder_states])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(lstm_state_size,), \n",
    "                              name='decoder_state_input_h')\n",
    "decoder_state_input_c = Input(shape=(lstm_state_size,), \n",
    "                              name='decoder_state_input_c')\n",
    "decoder_hidden_state_input = Input(shape=(max_words_text,lstm_state_size))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_output, decoder_state_output_h, decoder_state_output_c = connect_decoding_layer(decoder_state_inputs)\n",
    "\n",
    "model_deocder = Model(inputs=[decoder_input,decoder_hidden_state_input] + decoder_state_inputs,\n",
    "                      outputs=[decoder_output, decoder_state_output_h, decoder_state_output_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function Used to Predict the Summary of Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_text):\n",
    "    input_ints = word2int(input_text, word2int_lookup)\n",
    "    input_ints = pad_sequences(input_ints, maxlen=int(max_words_text), padding='pre')\n",
    "    \n",
    "    # Obtain the output and states of the encoder using the input text\n",
    "    encoder_out, encoder_state_h, encoder_state_c = model_encoder.predict(input_ints)\n",
    "    encoder_outputs = [encoder_out, encoder_state_h, encoder_state_c]\n",
    "   \n",
    "    # Data that will be fed into the decoder and initialize sample token to start token\n",
    "    decoder_input_data = np.zeros(shape=(1,max_words_text), dtype=np.int)\n",
    "    sampled_token_int = word2int_lookup[start_token]\n",
    "    \n",
    "    # Initialize predicted text and keep track of number of words processed\n",
    "    predicted_summary = list()\n",
    "    predicted_summary.append(start_token)\n",
    "    count_tokens = 0\n",
    "    \n",
    "    while sampled_token_int != word2int_lookup[end_token] and count_tokens < max_words_text:\n",
    "        # add the next token to the input data\n",
    "        decoder_input_data[0, count_tokens] = sampled_token_int\n",
    "        \n",
    "        input_data = {\n",
    "            'decoder_initial_state' : encoder_outputs,\n",
    "            'decoder_input' : decoder_input_data\n",
    "        }\n",
    "        \n",
    "        # use decoder to get output tokens\n",
    "        output_tokens, _, _ = model_decoder.predict(input_data)\n",
    "        \n",
    "        # get last predicted token as one hot array\n",
    "        last_token = output_tokens[0, count_tokens, :]\n",
    "        \n",
    "        # convert to int\n",
    "        sampled_token_int = np.argmax(last_token)\n",
    "        \n",
    "        # convert to word\n",
    "        sampled_word = int2word_lookup[sampled_token_int]\n",
    "        \n",
    "        # add to predicted summary\n",
    "        predicted_summary.append(sampled_word)\n",
    "        count_tokens += 1\n",
    "    \n",
    "    predicted_summary = \" \".join(predicted_summary)\n",
    "    \n",
    "    print(predicted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
